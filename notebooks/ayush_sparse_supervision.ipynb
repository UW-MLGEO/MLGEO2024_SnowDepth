{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ayushg/miniforge3/envs/deep-snow/lib/python3.12/site-packages/torchvision/io/image.py:14: UserWarning: Failed to load image Python extension: 'dlopen(/Users/ayushg/miniforge3/envs/deep-snow/lib/python3.12/site-packages/torchvision/image.so, 0x0006): Library not loaded: @rpath/libjpeg.9.dylib\n",
      "  Referenced from: <367D4265-B20F-34BD-94EB-4F3EE47C385B> /Users/ayushg/miniforge3/envs/deep-snow/lib/python3.12/site-packages/torchvision/image.so\n",
      "  Reason: tried: '/Users/ayushg/miniforge3/envs/deep-snow/lib/python3.12/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/Users/ayushg/miniforge3/envs/deep-snow/lib/python3.12/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/Users/ayushg/miniforge3/envs/deep-snow/lib/python3.12/lib-dynload/../../libjpeg.9.dylib' (no such file), '/Users/ayushg/miniforge3/envs/deep-snow/bin/../lib/libjpeg.9.dylib' (no such file)'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.path.dirname(os.path.abspath(os.getcwd())))\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from glob import glob\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import xarray as xr\n",
    "import rasterio as rio\n",
    "import rioxarray\n",
    "import math\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import deep_snow.models\n",
    "from deep_snow.utils import calc_norm, undo_norm, calc_dowy\n",
    "from deep_snow.dataset import norm_dict\n",
    "from tqdm import tqdm\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Random Grid for Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 740,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgrid = 30\n",
    "ygrid = 4\n",
    "X_coords = np.arange(np.random.randint(-160, -140), 128+160, xgrid)\n",
    "Y_coords = np.arange(np.random.randint(-160, -140), 128+160, ygrid)\n",
    "xv, yv = np.meshgrid(X_coords, Y_coords)\n",
    "xv, yv = xv.flatten(), yv.flatten()\n",
    "\n",
    "rot = np.deg2rad(25)\n",
    "\n",
    "xv_rot = (xv*np.cos(rot) + yv*np.sin(rot)).astype('int')\n",
    "yv_rot = (yv*np.cos(rot) - xv*np.cos(rot)).astype('int')\n",
    "\n",
    "mask1 = (xv>0)&(xv<128)&(yv>0)&(yv<128)\n",
    "mask2 = (xv_rot>0)&(xv_rot<128)&(yv_rot>0)&(yv_rot<128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 744,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "axs[0].scatter(xv[mask1], yv[mask1], s=10)\n",
    "axs[1].scatter(xv_rot[mask2], yv_rot[mask2], s=10)\n",
    "\n",
    "axs[0].set_title('Sampling Grid')\n",
    "axs[1].set_title('Rotated Sampling Grid')\n",
    "axs[0].set_xlim(0, 128)\n",
    "axs[1].set_xlim(0, 128)\n",
    "axs[0].set_ylim(0, 128)\n",
    "axs[1].set_ylim(0, 128)\n",
    "\n",
    "plt.savefig('sampling_data.png', bbox_inches='tight',\n",
    "            dpi=300, transparent=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving to pickle file of Random Sampling Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 745,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_grid(grid_sz_x=30, grid_sz_y=30):\n",
    "    # Hard coded patch size\n",
    "    X_coords = np.arange(np.random.randint(-160, -140), 128+160, grid_sz_x)\n",
    "    Y_coords = np.arange(np.random.randint(-160, -140), 128+160, grid_sz_y)\n",
    "    xv, yv = np.meshgrid(X_coords, Y_coords)\n",
    "    xv, yv = xv.flatten(), yv.flatten()\n",
    "\n",
    "    rot = np.deg2rad(np.random.randint(0, 50)) \n",
    "\n",
    "    xv_rot = (xv*np.cos(rot) + yv*np.sin(rot)).astype('int')\n",
    "    yv_rot = (yv*np.cos(rot) - xv*np.cos(rot)).astype('int')\n",
    "    mask2 = (xv_rot>0)&(xv_rot<128)&(yv_rot>0)&(yv_rot<128)\n",
    "\n",
    "    return xv_rot[mask2], yv_rot[mask2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 747,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_list = {}\n",
    "xgrid, ygrid = 30, 4\n",
    "for i, file in enumerate(files):\n",
    "    filenm = file.split('/')[-1].replace('.nc', '')\n",
    "    xv, yv = random_grid(grid_sz_x=xgrid, grid_sz_y=ygrid)\n",
    "    dataset_list[filenm] = { 'x_idx': list(xv), 'y_idx': list(yv) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "out_path = f'/Users/ayushg/Desktop/Courses/ML_GEO/mlgeo-2024-deep-snow/final_data/jack_subsets 2/dataset_index_{xgrid}_{ygrid}.pkl'\n",
    "with open(out_path, \"wb\") as fp:\n",
    "    pickle.dump(dataset_list, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 749,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dictionary pkl file\n",
    "with open(out_path, 'rb') as fp:\n",
    "    dataset_idx = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 750,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length of indices 160\n"
     ]
    }
   ],
   "source": [
    "lens = []\n",
    "for key in dataset_idx.keys():\n",
    "    xi = dataset_idx[key]['x_idx']\n",
    "    lens.append(len(xi))\n",
    "\n",
    "print(\"Max length of indices\", max(lens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training using Sparse Supervision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating dataloader using sampling points\n",
    "\n",
    "We are going to sample the sparse points during training, while during test and validation we will be sampling the entire image to evaludate the performance on the entire image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_dict = {'aso_sd':[0, 25],\n",
    "             'vv':[-59, 30],\n",
    "             'vh':[-65, 17],\n",
    "             'cr':[-43, 16],\n",
    "             'delta_cr':[-33, 27],\n",
    "             'AOT':[0, 572],\n",
    "             'coastal':[0, 24304],\n",
    "             'blue':[0, 23371],\n",
    "             'green':[0, 26440],\n",
    "             'red':[0, 21576],\n",
    "             'red_edge1':[0, 20796],\n",
    "             'red_edge2':[0, 20432],\n",
    "             'red_edge3':[0, 20149],\n",
    "             'nir':[0, 21217],\n",
    "             'water_vapor':[0, 18199],\n",
    "             'swir1':[0, 17669],\n",
    "             'swir2':[0, 17936],\n",
    "             'scene_class_map':[0, 15],\n",
    "             'water_vapor_product':[0, 6518],\n",
    "             'elevation':[-100, 9000],\n",
    "             'aspect':[0, 360],\n",
    "             'slope':[0, 90],\n",
    "             'curvature':[-22, 22],\n",
    "             'tpi':[-164, 167],\n",
    "             'tri':[0, 913],\n",
    "             'latitude':[-90, 90],\n",
    "             'longitude':[-180, 180],\n",
    "             'dowy': [0, 365]}\n",
    "\n",
    "def compute_maxlen(out_path):\n",
    "    # Read dictionary pkl file\n",
    "    with open(out_path, 'rb') as fp:\n",
    "        dataset_idx = pickle.load(fp)\n",
    "    lens = []\n",
    "\n",
    "    for key in dataset_idx.keys():\n",
    "        xi = dataset_idx[key]['x_idx']\n",
    "        lens.append(len(xi))\n",
    "\n",
    "    return max(lens)\n",
    "\n",
    "\n",
    "# Process each file and normalize the relevant features\n",
    "def process_file(file_path):\n",
    "    ds = xr.open_dataset(file_path)\n",
    "    # Normalize features using the norm_dict\n",
    "    data_dict = {}\n",
    "    data_dict['latitude'] = calc_norm(torch.Tensor(ds['latitude'].values), norm_dict['latitude'])\n",
    "    data_dict['longitude'] = calc_norm(torch.Tensor(ds['longitude'].values), norm_dict['longitude'])\n",
    "    data_dict['elevation'] = calc_norm(torch.Tensor(ds['elevation'].values), norm_dict['elevation'])\n",
    "    data_dict['slope'] = calc_norm(torch.Tensor(ds['slope'].values), norm_dict['slope'])\n",
    "    data_dict['tri'] = calc_norm(torch.Tensor(ds['tri'].values), norm_dict['tri'])\n",
    "    data_dict['tpi'] = calc_norm(torch.Tensor(ds['tpi'].values), norm_dict['tpi'])\n",
    "    data_dict['dowy'] = calc_norm(torch.Tensor(ds['dowy'].values), norm_dict['dowy'])\n",
    "    # Reshape PC components to 2D\n",
    "    s1_pc1_2d = ds['s1_pc1'].values.reshape(128, 128)\n",
    "    s1_pc2_2d = ds['s1_pc2'].values.reshape(128, 128)\n",
    "    s2_pc1_2d = ds['s2_pc1'].values.reshape(128, 128)\n",
    "    s2_pc2_2d = ds['s2_pc2'].values.reshape(128, 128)\n",
    "    s2_pc3_2d = ds['s2_pc3'].values.reshape(128, 128)\n",
    "    # Stack normalized features\n",
    "    features = np.stack([\n",
    "        data_dict['elevation'].numpy(),\n",
    "        data_dict['slope'].numpy(),\n",
    "        data_dict['tri'].numpy(),\n",
    "        data_dict['tpi'].numpy(),\n",
    "        data_dict['latitude'].numpy(),\n",
    "        data_dict['longitude'].numpy(),\n",
    "        s1_pc1_2d,\n",
    "        s1_pc2_2d,\n",
    "        s2_pc1_2d,\n",
    "        s2_pc2_2d,\n",
    "        s2_pc3_2d,\n",
    "        data_dict['dowy'].numpy()\n",
    "    ], axis=0)\n",
    "    target = ds['aso_sd'].values\n",
    "    return features, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# For computing mean and std for normalisation of snow_depth\n",
    "# And potentially using histogram distribution of these points for weighted loss (penalising the model more on less frequent values compared to more frequent values)\n",
    "def compute_train_mean_std(ds_train, num_samples=None, num_workers=8, batch_size=128, max_len=160, return_distribution=False, bin_edges=None, collate_fn=None):\n",
    "    \"\"\"\n",
    "    Compute training statistics (mean and standard deviation) per tensor channel (with channel axis=1).\n",
    "    Can be used for multi-channel inputs and single channel targets\n",
    "    Args:\n",
    "        ds_train: torch dataset\n",
    "        data_key: string name of torch tensor with shape (channels, height, width). (e.g. 'inputs', 'labels_mean')\n",
    "        num_samples: max number of patches used to calculate the statistics\n",
    "        num_workers: number of workers used in dataloader\n",
    "        return_distribution: bool, if True: returns the number of samples per bin\n",
    "        bin_edges: used to return the number of sampler per bin\n",
    "\n",
    "    Returns:\n",
    "        train_mean: numpy array with shape (channels,)\n",
    "        train_std: numpy array with shape (channels,)\n",
    "    \"\"\"\n",
    "    max_range = 1.7e308  # max range for float64\n",
    "\n",
    "    dl_train = DataLoader(ds_train, batch_size=32, shuffle=True, collate_fn=lambda b: collate_fn(b, False, True, max_len))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # get the dimensions from the first sample\n",
    "        for batch_idx, batch in enumerate(dl_train):\n",
    "            _, first_sample, _ = batch\n",
    "\n",
    "            _, channels, length = first_sample.unsqueeze(1).shape\n",
    "            break\n",
    "        print('channels, height, width: ', channels, length)\n",
    "    \n",
    "        # init torch tensors\n",
    "        sum_ = torch.zeros(channels, dtype=torch.float64)\n",
    "        sum_sq_ = torch.zeros(channels, dtype=torch.float64)\n",
    "        count_ = torch.zeros(1, dtype=torch.float64)  # assume equal count for each channel (image or labels)\n",
    "\n",
    "        if return_distribution:\n",
    "            if bin_edges is None:\n",
    "                bin_edges = np.arange(0, 101, 1)\n",
    "            bin_num_samples = np.zeros(len(bin_edges)-1, dtype=np.float64)\n",
    "\n",
    "        for batch_idx, batch in enumerate(tqdm(dl_train, ncols=100, desc='stats')):  # for each training step\n",
    "            \n",
    "            _, Y, index = batch\n",
    "            # stop when num_samples are sampled\n",
    "            if batch_idx >= int(num_samples/batch_size):\n",
    "                print('Loaded {} samples to compute channel statistics.'.format(num_samples))\n",
    "                break\n",
    "\n",
    "            sum_ = sum_ + torch.nansum(Y, axis=(0, 1))\n",
    "            sum_sq_ = sum_sq_ + torch.nansum((Y ** 2), axis=(0, 1))\n",
    "            count_ = count_ + torch.sum(~torch.isnan(Y[:, ...]))  # use the first channel because we want the count per channel to compute image channel statistics\n",
    "\n",
    "            if return_distribution:\n",
    "                hist, bin_edges = np.histogram(Y, bins=bin_edges)\n",
    "                bin_num_samples = bin_num_samples + hist\n",
    "\n",
    "            # check overflow\n",
    "            if torch.any(sum_sq_ >= max_range):\n",
    "                print('sum_sq_: ', sum_sq_)\n",
    "                raise OverflowError('sum_sq_ is too large')\n",
    "\n",
    "        train_mean = sum_ / count_\n",
    "        train_mean_sq_ = sum_sq_ / count_\n",
    "        train_std = torch.sqrt(train_mean_sq_ - train_mean ** 2)\n",
    "\n",
    "        # cast to numpy float32\n",
    "        train_mean = train_mean.cpu().numpy().astype(np.float32)\n",
    "        train_std = train_std.cpu().numpy().astype(np.float32)\n",
    "\n",
    "    if return_distribution:\n",
    "        return train_mean, train_std, bin_num_samples, bin_edges\n",
    "    else:\n",
    "        return train_mean, train_std\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inverse_bin_frequency_weights(labels, bin_edges, bin_weights):\n",
    "    bin_indices = np.digitize(labels, bin_edges, right=False) - 1  # to start with index=0\n",
    "    bin_indices = np.clip(bin_indices, 0, len(bin_edges) - 2)\n",
    "   # handle nan labels: nans map to len()\n",
    "    nan_mask = np.isnan(labels)\n",
    "    bin_indices[nan_mask] = 0\n",
    "    # print(bin_indices, labels, bin_weights)\n",
    "    sample_weights = bin_weights[bin_indices].astype(np.float32)    \n",
    "    return sample_weights\n",
    "\n",
    "\n",
    "class SparseSnowDataset(Dataset):\n",
    "    \"\"\"Sparse Snow Depth dataset\"\"\"\n",
    "    def __init__(self, datapath, idx_filename, mode, get_target=False, get_weights=False, bin_edges=None, bin_weights=None):\n",
    "        self.datapath = datapath\n",
    "        self.data_files = glob(f'{datapath}/ncs/*.nc')\n",
    "\n",
    "        # Calculate the sizes for each split\n",
    "        train_size = int(0.7 * len(self.data_files))  # 70% for training\n",
    "        test_size = int(0.2 * len(self.data_files))   # 20% for testing\n",
    "        val_size = len(self.data_files) - train_size - test_size  # Remaining 10% for validation\n",
    "\n",
    "        # Setting modes\n",
    "        if mode=='train':\n",
    "            self.data_files = self.data_files[:train_size]\n",
    "        elif mode=='test':\n",
    "            self.data_files = self.data_files[train_size:train_size + test_size]\n",
    "        elif mode=='val':\n",
    "            self.data_files = self.data_files[train_size + test_size:]\n",
    "\n",
    "        # Read dictionary pkl file\n",
    "        with open(f'{datapath}/{idx_filename}', 'rb') as fp:\n",
    "            self.dataset_idx = pickle.load(fp)\n",
    "\n",
    "        self.data_file_name = {}\n",
    "        for file in tqdm(self.data_files):\n",
    "            features, target = process_file(file)\n",
    "            self.data_file_name[file] = {'features': features, 'target': target}\n",
    "\n",
    "        self.get_target = get_target\n",
    "        self.get_weights = get_weights\n",
    "        if get_weights:\n",
    "            self.bin_edges = bin_edges\n",
    "            self.bin_weights = bin_weights\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_files)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        file_name = self.data_files[index]\n",
    "        features, target = self.data_file_name[file_name]['features'], self.data_file_name[file_name]['target']\n",
    "        # features, target = process_file(file_name)\n",
    "        xy_grid = self.dataset_idx[file_name.split('/')[-1].replace('.nc', '')]\n",
    "        x_idx, y_idx = xy_grid['x_idx'], xy_grid['y_idx']\n",
    "        index = np.array((np.array(y_idx) * features.shape[-1]) + np.array(x_idx))\n",
    "        if self.get_target:\n",
    "            target = torch.gather(torch.flatten(torch.tensor(target), start_dim=0), 0, torch.tensor(index))\n",
    "            if self.get_weights:\n",
    "                sample_weights = get_inverse_bin_frequency_weights(target, self.bin_edges, self.bin_weights)\n",
    "                return features, target, index, sample_weights\n",
    "\n",
    "        return features, target, index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_IDX_LENGTH = 25\n",
    "\n",
    "def collate_fn(batch, get_weights=False, get_target=False, max_length=160):\n",
    "    features  = []\n",
    "    targets = []\n",
    "    idxs = []\n",
    "    weights = []\n",
    "    for batch_ele in batch:\n",
    "        if get_weights:\n",
    "            feature, target, idx, weight = batch_ele\n",
    "        else:\n",
    "            feature, target, idx = batch_ele\n",
    "\n",
    "        features.append(feature)\n",
    "        padding = max_length - idx.shape[0]\n",
    "        if padding > 0:\n",
    "            idxs.append(F.pad(torch.tensor(idx), (0, padding), \"constant\", 0))\n",
    "            if get_weights:\n",
    "                weights.append(F.pad(torch.tensor(weight), (0, padding), \"constant\", 0))\n",
    "            if get_target:\n",
    "                targets.append(F.pad(target, (0, padding), \"constant\", 0))\n",
    "            else:\n",
    "                targets.append(torch.tensor(target, dtype=torch.float))\n",
    "        else:\n",
    "            idxs.append(torch.tensor(idx))\n",
    "            if get_weights:\n",
    "                weights.append(torch.tensor(weight))\n",
    "            if get_target:\n",
    "                targets.append(torch.tensor(target))\n",
    "            else:\n",
    "                targets.append(torch.tensor(target, dtype=torch.float))\n",
    "    \n",
    "    if get_weights:\n",
    "        return torch.tensor(features, dtype=torch.float), torch.stack(targets), torch.stack(idxs), torch.stack(weights)\n",
    "    if get_target:\n",
    "        return torch.tensor(features, dtype=torch.float), torch.stack(targets), torch.stack(idxs)\n",
    "    else:\n",
    "        return torch.tensor(features, dtype=torch.float), torch.stack(targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2478/2478 [02:39<00:00, 15.55it/s]\n",
      "/var/folders/q7/4r0lxc6j59lf_bjt9p2p0y8h0000gp/T/ipykernel_1275/2111274140.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  targets.append(torch.tensor(target))\n",
      "/var/folders/q7/4r0lxc6j59lf_bjt9p2p0y8h0000gp/T/ipykernel_1275/2111274140.py:36: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1729646995093/work/torch/csrc/utils/tensor_new.cpp:281.)\n",
      "  return torch.tensor(features, dtype=torch.float), torch.stack(targets), torch.stack(idxs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "channels, height, width:  1 160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "stats:  24%|                                          | 19/78 [00:18<00:57,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2478 samples to compute channel statistics.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2478/2478 [03:29<00:00, 11.84it/s] \n",
      "100%|| 354/354 [00:22<00:00, 15.90it/s]\n",
      "100%|| 708/708 [00:45<00:00, 15.59it/s]\n"
     ]
    }
   ],
   "source": [
    "# Creating train, validation, and test dataloaders\n",
    "data_path = '/Users/ayushg/Desktop/Courses/ML_GEO/mlgeo-2024-deep-snow/final_data/jack_subsets 2'\n",
    "idx_fn = 'dataset_index_30_4.pkl'\n",
    "\n",
    "max_len = compute_maxlen(f'{data_path}/{idx_fn}')\n",
    "train_snow_dataset = SparseSnowDataset(data_path, idx_fn, 'train', get_target=True)\n",
    "target_mean, target_std, bin_num_samples, bin_edges = compute_train_mean_std(train_snow_dataset, max_len=max_len, bin_edges=np.arange(0, 6, 0.5), num_samples=len(train_snow_dataset), return_distribution=True, collate_fn=collate_fn)\n",
    "bin_weights = np.sqrt(1/bin_num_samples)/np.sqrt(1/bin_num_samples).sum()\n",
    "\n",
    "train_snow_dataset = SparseSnowDataset(data_path, idx_fn, 'train', get_target=True, get_weights=True, bin_edges=bin_edges, bin_weights=bin_weights)\n",
    "val_snow_dataset = SparseSnowDataset(data_path, idx_fn, 'val', get_target=False, get_weights=False)\n",
    "test_snow_dataset = SparseSnowDataset(data_path, idx_fn, 'test', get_target=False, get_weights=False)\n",
    "\n",
    "# # Create data loaders\n",
    "train_loader = DataLoader(train_snow_dataset, batch_size=32, shuffle=True, collate_fn=lambda b: collate_fn(b, True, True, max_len))\n",
    "test_loader = DataLoader(test_snow_dataset, batch_size=32, shuffle=False, collate_fn=lambda b: collate_fn(b, False, False, max_len))\n",
    "val_loader = DataLoader(val_snow_dataset, batch_size=32, shuffle=False, collate_fn=lambda b: collate_fn(b, False, False, max_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture\n",
    "\n",
    "We are just picking a simple CNN for evaluating if sparse supervision helps or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SnowDepthCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SnowDepthCNN, self).__init__()\n",
    "        # First conv layer: (12, 128, 128) -> (32, 128, 128)\n",
    "        self.conv1 = nn.Conv2d(in_channels=12, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        # Second conv layer: (32, 128, 128) -> (16, 128, 128)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=16, kernel_size=3, padding=1)\n",
    "        # Final conv layer: (16, 128, 128) -> (1, 128, 128)\n",
    "        self.pred = nn.Conv2d(in_channels=16, out_channels=1, kernel_size=1)\n",
    "        self.var = nn.Sequential(nn.Conv2d(in_channels=16, out_channels=1, kernel_size=1),\n",
    "                                 nn.ReLU(inplace=False))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        pred = self.pred(x)\n",
    "        var = self.var(x)\n",
    "        return pred.squeeze(1), var.squeeze(1)  # Remove channel dimension to match target shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianNLL(nn.Module):\n",
    "    \"\"\"\n",
    "    Gaussian negative log likelihood to fit the mean and variance to p(y|x)\n",
    "    Note: We estimate the heteroscedastic variance. Hence, we include the var_i of sample i in the sum\n",
    "    over all samples N. Furthermore, the constant log term is discarded.\n",
    "    \"\"\"\n",
    "    def __init__(self, reduction='mean'):\n",
    "        super(GaussianNLL, self).__init__()\n",
    "        self.eps = 1e-8\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def __call__(self, prediction, variance, target, mask=None):\n",
    "        variance = variance + self.eps\n",
    "        if self.reduction == 'mean':\n",
    "            return torch.mean((0.5 / variance * (prediction - target)**2 + 0.5 * torch.log(variance))[~mask])\n",
    "        elif self.reduction == 'none':\n",
    "            return 0.5 / variance * (prediction - target)**2 + 0.5 * torch.log(variance)\n",
    "\n",
    "\n",
    "class SparseMAE(nn.Module):\n",
    "    def __init__(self, reduction='mean'):\n",
    "        super(SparseMAE, self).__init__()\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def __call__(self, prediction, variance, target, mask):\n",
    "        if self.reduction == 'mean':\n",
    "            return torch.mean(F.l1_loss(prediction, target, reduction='none')[~mask])\n",
    "        elif self.reduction == 'none':\n",
    "            return F.l1_loss(prediction, target, reduction='none')\n",
    "\n",
    "\n",
    "class SampleWeightedLoss(nn.Module):\n",
    "    '''\n",
    "    Weighted loss are computed to account for the class imbalance in data\n",
    "    Less frequent values are penalized more compared to more frequent values\n",
    "    '''\n",
    "    def __init__(self, loss_key='MAE', norm_batch=True):\n",
    "        super(SampleWeightedLoss, self).__init__()\n",
    "        self.loss_key = loss_key\n",
    "\n",
    "        if loss_key == 'MSE':\n",
    "            self.loss_fun = torch.nn.MSELoss(reduction='none')\n",
    "        elif loss_key == 'MAE':\n",
    "            self.loss_fun = torch.nn.L1Loss(reduction='none')\n",
    "        elif loss_key == 'GNLL':\n",
    "            self.loss_fun = GaussianNLL(reduction='none')\n",
    "        else:\n",
    "            raise ValueError('Sample weighted loss is not yet implemented for loss_key={}'.format(loss_key))\n",
    "\n",
    "    def __call__(self, output, target, sample_weights=None, variance=None, mask=None):\n",
    "        if self.loss_key!='GNLL':\n",
    "            loss_values = self.loss_fun(output, target)\n",
    "        else:\n",
    "            loss_values = self.loss_fun(output, variance, target)\n",
    "        \n",
    "        if mask is None:\n",
    "            loss = torch.mul(loss_values, sample_weights)[~mask].sum()\n",
    "        else:\n",
    "            loss = loss_values.mean()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 796,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(model, train_loader, val_loader, test_loader, criterion_train, criterion_test, optimizer, num_epochs, device, norm_mean, norm_std):\n",
    "    \"\"\"\n",
    "    Train, validate, and test the model.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The PyTorch model to train and evaluate.\n",
    "        train_loader (DataLoader): DataLoader for the training dataset.\n",
    "        val_loader (DataLoader): DataLoader for the validation dataset.\n",
    "        test_loader (DataLoader): DataLoader for the test dataset.\n",
    "        criterion (Loss): The loss function.\n",
    "        optimizer (Optimizer): The optimizer for model training.\n",
    "        num_epochs (int): Number of training epochs.\n",
    "        device (torch.device): Device to use for computation.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing training losses, validation losses, and final test loss.\n",
    "    \"\"\"\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    get_weight = True\n",
    "    norm_std = torch.tensor(norm_std).to(device)\n",
    "    norm_mean = torch.tensor(norm_mean).to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch in tqdm(train_loader, desc=f'Epoch {epoch+1}'):\n",
    "            if get_weight:\n",
    "                features, targets, index, weights = batch\n",
    "                features, targets, index, weights = features.to(device), targets.to(device), index.to(device), weights.to(device)\n",
    "\n",
    "            targets = (targets - norm_mean)/norm_std\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            preds, variances = model(features)\n",
    "\n",
    "            # Getting sparse elements from array\n",
    "            mask = (index[:,:]<=0)\n",
    "            sparse_pred = torch.gather(torch.flatten(preds, start_dim=1), 1, index)\n",
    "            sparse_var = torch.gather(torch.flatten(variances, start_dim=1), 1, index)\n",
    "            # sparse_target = torch.gather(torch.flatten(targets, start_dim=1), 1, index)\n",
    "\n",
    "            loss = criterion_train(sparse_pred, targets, weights, sparse_var, mask=mask)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        # It can be performed on entire target data (ASO Snow Depth)\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for features, targets in val_loader:\n",
    "                features, targets = features.to(device), targets.to(device)\n",
    "                outputs, _ = model(features)\n",
    "                outputs = outputs*norm_std + norm_mean\n",
    "                val_loss += criterion_test(outputs, targets).item()\n",
    "        \n",
    "        # Compute average losses\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}]')\n",
    "        print(f'Training Loss: {avg_train_loss:.4f}')\n",
    "        print(f'Validation Loss: {avg_val_loss:.4f}')\n",
    "    \n",
    "    # Testing\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for features, targets in test_loader:\n",
    "            features, targets = features.to(device), targets.to(device)\n",
    "            \n",
    "            outputs, _ = model(features)\n",
    "            test_loss += criterion_test(outputs, targets).item()\n",
    "\n",
    "    final_test_loss = test_loss / len(test_loader)\n",
    "    print(f'\\nFinal Test Loss: {final_test_loss:.4f}')\n",
    "\n",
    "    return train_losses, val_losses, final_test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   4%|         | 3/78 [00:02<01:01,  1.22it/s]/var/folders/q7/4r0lxc6j59lf_bjt9p2p0y8h0000gp/T/ipykernel_21442/2111274140.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  targets.append(torch.tensor(target))\n",
      "Epoch 1: 100%|| 78/78 [01:02<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10]\n",
      "Training Loss: 4009622087.2051\n",
      "Validation Loss: 0.8237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|| 78/78 [01:02<00:00,  1.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10]\n",
      "Training Loss: 443236.6814\n",
      "Validation Loss: 0.8235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|| 78/78 [01:01<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10]\n",
      "Training Loss: 983.4651\n",
      "Validation Loss: 0.8235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|| 78/78 [01:01<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10]\n",
      "Training Loss: 947.4884\n",
      "Validation Loss: 0.8235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|| 78/78 [01:01<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10]\n",
      "Training Loss: 944.2229\n",
      "Validation Loss: 0.8235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|| 78/78 [01:02<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10]\n",
      "Training Loss: 940.0411\n",
      "Validation Loss: 0.8234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|| 78/78 [01:01<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10]\n",
      "Training Loss: 935.4200\n",
      "Validation Loss: 0.8234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|| 78/78 [01:01<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10]\n",
      "Training Loss: 930.2187\n",
      "Validation Loss: 0.8234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|| 78/78 [01:02<00:00,  1.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10]\n",
      "Training Loss: 924.6778\n",
      "Validation Loss: 0.8234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|| 78/78 [01:02<00:00,  1.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10]\n",
      "Training Loss: 918.7305\n",
      "Validation Loss: 0.8234\n",
      "\n",
      "Final Test Loss: 0.6409\n"
     ]
    }
   ],
   "source": [
    "model = SnowDepthCNN()\n",
    "\n",
    "loss_sample = SampleWeightedLoss(loss_key='GNLL')\n",
    "loss_mae = nn.L1Loss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 10\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "# device = 'cpu'\n",
    "model = model.to(device)\n",
    "print(device)\n",
    "\n",
    "train_losses2, val_losses2, test_loss2 = train_and_evaluate_model(\n",
    "    model=model.to(device),\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,  \n",
    "    test_loader=test_loader,\n",
    "    criterion_train=loss_sample,\n",
    "    criterion_test=loss_mae,\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=num_epochs,\n",
    "    device=device,\n",
    "    norm_mean=target_mean, norm_std=target_std\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-snow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
